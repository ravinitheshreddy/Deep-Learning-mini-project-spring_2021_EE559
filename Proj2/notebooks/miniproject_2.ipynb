{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "developed-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (for the framework)\n",
    "import torch\n",
    "import math\n",
    "# Imports (for plotting)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "african-carnival",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x22ddcba3f70>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#switching off autograd globally\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electrical-berkeley",
   "metadata": {},
   "source": [
    "# 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-christmas",
   "metadata": {},
   "source": [
    "## Baseclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interested-storm",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Base class for all modules.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Function to get the input, apply forward pass of module and\n",
    "        returns a tensor or a tuple of tensors.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradswrtoutput):\n",
    "        \"\"\"\n",
    "        Function to get the input gradient of the loss with respect to the\n",
    "        module’s output, accumulate the gradient wrt the parameters, and\n",
    "        return a tensor or a tuple of tensors containing the gradient of\n",
    "        the loss wrt the module’s input.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Returns a list of pairs, each composed of a parameter tensor, and\n",
    "        a gradient tensor of same size.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradients of a module to 0\n",
    "        \"\"\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-reservoir",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naked-lindsay",
   "metadata": {},
   "source": [
    "### TanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "greek-verification",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Module):\n",
    "\n",
    "    \"\"\"Module to apply the Hyperbolic Tangent function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.name = \"TanH\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying tanh to the input\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the tanh should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the tanh on the input\n",
    "        \"\"\"\n",
    "\n",
    "        self.out_ = input_.tanh()\n",
    "\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying tanh\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "\n",
    "        return gradwrtoutput * (1 - self.out_.pow(2))\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-model",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "premium-split",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "\n",
    "    \"\"\"Module to apply the Rectified Linear function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.name = \"ReLU\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying ReLU to the input.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the ReLU should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the ReLU on the input\n",
    "        \"\"\"\n",
    "\n",
    "        self.out = input_.clamp(min=0.0)\n",
    "\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying ReLU\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "\n",
    "        self.out[self.out <= 0] = 0\n",
    "        self.out[self.out > 0] = 1\n",
    "\n",
    "        return gradwrtoutput * self.out\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-citizen",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "divided-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Module):\n",
    "\n",
    "    \"\"\"Module to apply the Sigmoid function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super().__init__()\n",
    "        self.name = \"Sigmoid\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying sigmoid to the input.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the sigmoid should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the sigmoid on the input\n",
    "        \"\"\"\n",
    "\n",
    "        self.out_ = input_.sigmoid()\n",
    "\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying sigmoid\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "\n",
    "        return gradwrtoutput * (self.out_ - self.out_**2)\n",
    "\n",
    "    def param(self):\n",
    "\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-horse",
   "metadata": {},
   "source": [
    "## Batch Norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-curve",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-times",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "silver-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "\n",
    "    \"\"\"Module to calculate the Mean Squared Error.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.name = \"MSE Loss\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def forward(self, output_: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the MSE Loss between output_ and target\n",
    "\n",
    "        Parameters:\n",
    "            output_ (Tensor): First tensor to calculate the MSE.\n",
    "            target (Tensor): Second tensor to calculate the MSE.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The Mean Squared Loss between input_ and target\n",
    "        \"\"\"\n",
    "\n",
    "        self.error = output_ - target\n",
    "        self.loss = self.error.pow(2).mean()\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        gradient of loss\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of Mean Squared Loss between input_ and target\n",
    "        \"\"\"\n",
    "\n",
    "        return (2 * self.error)/self.error.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-respect",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "activated-mobile",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "\n",
    "    \"\"\"\n",
    "    Base class for optimizers.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "         Perform the single optimization step\n",
    "        \"\"\"\n",
    "\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifth-latin",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "brilliant-comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "\n",
    "    \"\"\"\n",
    "    Module to perform Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params: list, lr=0.01):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            params (list): List of the parameters of the network\n",
    "            lr (float): The learning rate of the network\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.name = \"SGD\"\n",
    "\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "        if self.lr <= 0.0:\n",
    "            raise ValueError(\n",
    "                \"Learning rate {} should be greater than zero\".format(self.lr))\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Function to perform the single optimization step\n",
    "        \"\"\"\n",
    "\n",
    "        for weight, gradient in self.params:\n",
    "            if (weight is None) or (gradient is None):\n",
    "                # incase of activation function modules, skip them\n",
    "                continue\n",
    "            else:\n",
    "                weight.add_(-self.lr*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minus-baghdad",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-mountain",
   "metadata": {},
   "source": [
    "### Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "formed-knight",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "\n",
    "    \"\"\"\n",
    "    Module that implements a linear matrix operation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int,\n",
    "                 bias: bool = True, weightsinit: str = \"uniform\"):\n",
    "        \"\"\"\n",
    "        Initialises the layer by creating empty weight and bias tensors\n",
    "        and initialising them using uniform distribution.\n",
    "\n",
    "        Parameters:\n",
    "            in_features (int): The size of each input sample\n",
    "            out_features (int): The size of each output sample\n",
    "            bias – If set to False, the layer will not learn an additive bias. Default: True\n",
    "            weightsinit (str): The type of weight initialization to use\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.name = \"Linear\"\n",
    "\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "        self.weightsinit = weightsinit\n",
    "\n",
    "        self.w = torch.empty(self.in_features, self.out_features)\n",
    "        self.gradw = torch.empty(self.in_features, self.out_features)\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = torch.empty(self.out_features)\n",
    "            self.gradb = torch.empty(self.out_features)\n",
    "        else:\n",
    "            self.b = None\n",
    "            self.gradb = None\n",
    "\n",
    "        self.initWeights()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def initWeights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weight and bias parameters of the layer depending on\n",
    "        the weightinit parameter. Irrespective of the weightinit parameter\n",
    "        the bias are always zero initialised \n",
    "\n",
    "        If \"weightsinit\" is\n",
    "            1. uniform (by default), the weights are initiliased using uniform distribution.\n",
    "            2. xavier, the weights are initiliased using Xavier Initialisation.\n",
    "            3. kaiming, the weights are initiliased using Kaiming Initialisation when using ReLU layer\n",
    "        \"\"\"\n",
    "\n",
    "        if self.weightsinit == \"uniform\":\n",
    "            k=math.sqrt(1.0/self.in_features)\n",
    "            self.w.uniform_(-k,k)\n",
    "\n",
    "        elif self.weightsinit == \"xavier\":\n",
    "            self.w.normal_(0,math.sqrt(2/(self.in_features + self.out_features)))\n",
    "\n",
    "        elif self.weightsinit == \"kaiming\":\n",
    "            self.w.normal_(0,math.sqrt(2/(self.in_features)))\n",
    "\n",
    "        self.gradw.fill_(0)\n",
    "        \n",
    "        if self.b is not None:\n",
    "            self.b.fill_(0)\n",
    "            self.gradb.fill_(0)\n",
    "\n",
    "                                       \n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the layer by multiplying the input with weights and adding the bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.inp = input_\n",
    "\n",
    "        if self.b is None:\n",
    "            self.output = self.inp.matmul(self.w)\n",
    "        else:\n",
    "            self.output = self.inp.matmul(self.w).add(self.b)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Computes the gradient for the weights and biases.\n",
    "        \"\"\"\n",
    "\n",
    "        gradw = self.inp.t().matmul(gradwrtoutput)\n",
    "        self.gradw.add_(gradw)\n",
    "\n",
    "        if self.b is not None:\n",
    "            gradb = gradwrtoutput.sum(0)\n",
    "            self.gradb.add_(gradb)\n",
    "        gradient = gradwrtoutput.matmul(self.w.t())\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def param(self) -> list:\n",
    "        \"\"\"\n",
    "        Returns the parameters of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        params = [(self.w, self.gradw)]\n",
    "        if self.b is not None:\n",
    "            params.append((self.b, self.gradb))\n",
    "\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient to zero\n",
    "        \"\"\"\n",
    "\n",
    "        self.gradw.zero_()\n",
    "\n",
    "        if self.b is not None:\n",
    "            self.gradb.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-precipitation",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "physical-savings",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Module to hold the layers and build the Network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            *args (list[Module]): The list of modules to be constructed in the\n",
    "            network.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.name = \"Sequential\"\n",
    "\n",
    "        # A list to hold all layers of the network\n",
    "        self.modules = [module for module in args]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Feed Forward prediction of the network. The input is propagated through\n",
    "        all the layers and the output of the final layer is returned.\n",
    "\n",
    "        Parameters:\n",
    "            input_ (Tensor): The input sample\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        # incase of no layers, the input itself is returned as output\n",
    "        output = input_\n",
    "\n",
    "        for module in self.modules:\n",
    "            output = module.forward(output)\n",
    "\n",
    "        self.output = output\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Backward propagation of the network. The error is propagated through\n",
    "        all the layers iteratively.\n",
    "\n",
    "        Parameters:\n",
    "            input_ (Tensor): The input sample\n",
    "        \"\"\"\n",
    "        # The error is propagated in the reverse (backward) direction\n",
    "        for module in reversed(self.modules):\n",
    "            gradwrtoutput = module.backward(gradwrtoutput)\n",
    "\n",
    "    def param(self) -> list:\n",
    "        \"\"\"\n",
    "        List of parameters of all modules\n",
    "\n",
    "        Returns:\n",
    "            params (list): List of tuple of weight and bias of each layer in\n",
    "            the network\n",
    "        \"\"\"\n",
    "\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.extend(module.param())\n",
    "\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient to zero of all modules\n",
    "        \"\"\"\n",
    "\n",
    "        for weight, gradient in self.param():\n",
    "            if (weight is None) or (gradient is None):\n",
    "                # incase of activation function modules, skip them\n",
    "                continue\n",
    "            else:\n",
    "                gradient.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-chorus",
   "metadata": {},
   "source": [
    "# 2. Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vietnamese-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_points: int) -> [torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Function to generate the dataset of 1,000 points sampled uniformly\n",
    "    in [0, 1]^2, each with a label 0 if outside the disk centered at (0.5; 0.5)\n",
    "    of radius 1/sqrt(2*pi), and 1 inside.\n",
    "\n",
    "    Parameters:\n",
    "        num_points (int): The number of points to be generated\n",
    "\n",
    "    Returns:\n",
    "        Tensor : A two dimensional input data with points sampled between [0,1]\n",
    "        Tensor : A two dimensional output data that contains labels\n",
    "        corresponding to the input data generated above as one hot encoded variable\n",
    "    \"\"\"\n",
    "\n",
    "    input_ = torch.Tensor(num_points, 2).uniform_(0, 1)\n",
    "\n",
    "    labels = input_.sub(0.5).pow(2).sum(1).sub(1 / (2 * math.pi)).sign().add(1).div(2).long()\n",
    "\n",
    "    labels_onehot = torch.empty(num_points, 2).fill_(0)\n",
    "    labels_onehot[:, 0] = labels == 0\n",
    "    labels_onehot[:, 1] = labels == 1\n",
    "\n",
    "    return input_, labels_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-vegetarian",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stupid-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Sequential, train_input: torch.Tensor,\n",
    "                train_target: torch.Tensor, loss_criteria: Module,\n",
    "                learning_rate: float, mini_batch_size: int, nb_epochs: int\n",
    "                ) -> list:\n",
    "    \"\"\"\n",
    "    Function to train a model and return the epoch wise loss as a list.\n",
    "\n",
    "    Parameters:\n",
    "        model (Sequential): The neural network model\n",
    "        train_input (Tensor): The input data samples\n",
    "        train_target (Tensor): The target of data samples\n",
    "        loss_criteria (Module): The loss function to use to train the model\n",
    "        learning_rate (float): The learning rate to be update the weights of\n",
    "        the model\n",
    "        mini_batch_size (int): The batch size to train the model\n",
    "        nb_epochs (int): The number of eppochs to train the network\n",
    "\n",
    "    Returns:\n",
    "        losses : A list of loss collected after each epoch of training\n",
    "    \"\"\"\n",
    "\n",
    "    optimizer = SGD(model.param(), lr=learning_rate)\n",
    "    losses = []\n",
    "    for epoch_number in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss_ = loss_criteria.forward(\n",
    "                output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            model.backward(loss_criteria.backward())\n",
    "            optimizer.step()\n",
    "        if epoch_number % 100 == 0:\n",
    "            print(\"\\tEpoch {} Training loss {}\".format(\n",
    "                epoch_number, loss_.item()))\n",
    "        losses.append(loss_.item())\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_nb_errors(model: Sequential, input_: torch.Tensor,\n",
    "                      target: torch.Tensor) -> int:\n",
    "    \"\"\"\n",
    "    Computes and returns the number of misclassifications done by the model\n",
    "\n",
    "    Parameters:\n",
    "        model (Sequential): The neural network model\n",
    "        input_ (Tensor): The input data samples\n",
    "        target (Tensor): The target of data samples\n",
    "\n",
    "    Returns:\n",
    "        nb_data_errors (int): The number of misclassifications.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    output = model.forward(input_)\n",
    "\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    _, actual = torch.max(target.data, 1)\n",
    "\n",
    "    for k in range(input_.size()[0]):\n",
    "        if actual.data[k] != predicted[k]:\n",
    "            nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-permission",
   "metadata": {},
   "source": [
    "# 4. Experiments\n",
    "\n",
    "## Data Generation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "radio-tolerance",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# generate the training and test samples\n",
    "\n",
    "train_input, train_target = generate_data(1000)\n",
    "test_input, test_target = generate_data(1000)\n",
    "\n",
    "# normalising the train and test data\n",
    "mu, std = train_input.mean(), train_input.std()\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)\n",
    "# avoid printing the result\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "friendly-puzzle",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We define the models to experiment with various parameters. All models will have an input layer, an output layer and 3 hidden layers with 25 neurons each. The factors that can be used for experiments are: The type of activation, the type of loss, the type of weight initialisation and whether or not to use batch normalisation.\n",
    "\n",
    "| **Model Number** | **Activation** | **Loss**      | **Weight Initialisation** |\n",
    "|:----------------:|:--------------:|:-------------:|:-------------------------:|\n",
    "| 1                | TanH           | MSE           | Uniform                   |\n",
    "| 2                | TanH           | MSE           | Xavier                    |\n",
    "| 3                | ReLU           | MSE           | Uniform                   |\n",
    "| 4                | ReLU           | MSE           | kaiming                   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "italian-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_1 = Sequential(Linear(2, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 2), Sigmoid())\n",
    "\n",
    "Model_2 = Sequential(Linear(2, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 2, weightsinit=\"xavier\"), Sigmoid())\n",
    "\n",
    "\n",
    "Model_3 = Sequential(Linear(2, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 2), Sigmoid())\n",
    "\n",
    "Model_4 = Sequential(Linear(2, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                     Linear(25, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                     Linear(25, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                     Linear(25, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                     Linear(25, 2, weightsinit=\"kaiming\"), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "applicable-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "mseloss = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "closing-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "          1: {\"model\": Model_1, \"activation\": \"TanH\", \"loss\": mseloss,\n",
    "              \"weightint\": \"Uniform\"},\n",
    "          2: {\"model\": Model_2, \"activation\": \"TanH\", \"loss\": mseloss,\n",
    "              \"weightint\": \"Xavier\"},\n",
    "          3: {\"model\": Model_3, \"activation\": \"ReLU\", \"loss\": mseloss,\n",
    "              \"weightint\": \"Uniform\"},\n",
    "          4: {\"model\": Model_4, \"activation\": \"ReLU\", \"loss\": mseloss,\n",
    "              \"weightint\": \"kaiming\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "about-context",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 with TanH activation, with Uniform weight initialisation \n",
      "\tEpoch 0 Training loss 0.24858123064041138\n",
      "\tEpoch 100 Training loss 0.24692125618457794\n",
      "\tEpoch 200 Training loss 0.2451481819152832\n",
      "\tEpoch 300 Training loss 0.24313151836395264\n",
      "\tEpoch 400 Training loss 0.24055691063404083\n",
      "\tEpoch 500 Training loss 0.23678691685199738\n",
      "\tEpoch 600 Training loss 0.23042067885398865\n",
      "\tEpoch 700 Training loss 0.2168087363243103\n",
      "\tEpoch 800 Training loss 0.15532158315181732\n",
      "\tEpoch 900 Training loss 0.06479392200708389\n",
      "\n",
      "Train Accuracy 97.50% 975/1000\n",
      "Test Accuracy 97.60% 976/1000\n",
      "Train Error 2.50% 25/1000\n",
      "Test Error 2.40% 24/1000\n",
      "\n",
      "\n",
      "Model 2 with TanH activation, with Xavier weight initialisation \n",
      "\tEpoch 0 Training loss 0.242956280708313\n",
      "\tEpoch 100 Training loss 0.22708025574684143\n",
      "\tEpoch 200 Training loss 0.1813233345746994\n",
      "\tEpoch 300 Training loss 0.10001980513334274\n",
      "\tEpoch 400 Training loss 0.055121682584285736\n",
      "\tEpoch 500 Training loss 0.03645309433341026\n",
      "\tEpoch 600 Training loss 0.027589723467826843\n",
      "\tEpoch 700 Training loss 0.022731466218829155\n",
      "\tEpoch 800 Training loss 0.019638195633888245\n",
      "\tEpoch 900 Training loss 0.017460964620113373\n",
      "\n",
      "Train Accuracy 99.40% 994/1000\n",
      "Test Accuracy 99.40% 994/1000\n",
      "Train Error 0.60% 6/1000\n",
      "Test Error 0.60% 6/1000\n",
      "\n",
      "\n",
      "Model 3 with ReLU activation, with Uniform weight initialisation \n",
      "\tEpoch 0 Training loss 0.25040924549102783\n",
      "\tEpoch 100 Training loss 0.24786622822284698\n",
      "\tEpoch 200 Training loss 0.24066044390201569\n",
      "\tEpoch 300 Training loss 0.19423125684261322\n",
      "\tEpoch 400 Training loss 0.08051437139511108\n",
      "\tEpoch 500 Training loss 0.038544993847608566\n",
      "\tEpoch 600 Training loss 0.024811653420329094\n",
      "\tEpoch 700 Training loss 0.019063517451286316\n",
      "\tEpoch 800 Training loss 0.01609623245894909\n",
      "\tEpoch 900 Training loss 0.014232665300369263\n",
      "\n",
      "Train Accuracy 99.70% 997/1000\n",
      "Test Accuracy 98.90% 989/1000\n",
      "Train Error 0.30% 3/1000\n",
      "Test Error 1.10% 11/1000\n",
      "\n",
      "\n",
      "Model 4 with ReLU activation, with kaiming weight initialisation \n",
      "\tEpoch 0 Training loss 0.23123353719711304\n",
      "\tEpoch 100 Training loss 0.09687192738056183\n",
      "\tEpoch 200 Training loss 0.03718052804470062\n",
      "\tEpoch 300 Training loss 0.022379085421562195\n",
      "\tEpoch 400 Training loss 0.01618807390332222\n",
      "\tEpoch 500 Training loss 0.013337736949324608\n",
      "\tEpoch 600 Training loss 0.011754683218896389\n",
      "\tEpoch 700 Training loss 0.010767948813736439\n",
      "\tEpoch 800 Training loss 0.009899952448904514\n",
      "\tEpoch 900 Training loss 0.009076934307813644\n",
      "\n",
      "Train Accuracy 99.70% 997/1000\n",
      "Test Accuracy 98.80% 988/1000\n",
      "Train Error 0.30% 3/1000\n",
      "Test Error 1.20% 12/1000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "model_result_attributes = {}\n",
    "mini_batch_size = 100\n",
    "\n",
    "for id_num, M in models.items():\n",
    "    print(\"Model {} with {} activation, with {} weight initialisation \".format(id_num, M[\"activation\"], M[\"weightint\"], str(M[\"loss\"])))\n",
    "\n",
    "    model = M[\"model\"]\n",
    "\n",
    "    model_loss = train_model(model, train_input, train_target, M[\"loss\"],\n",
    "                             learning_rate, mini_batch_size, nb_epochs)\n",
    "\n",
    "    nb_train_errors = compute_nb_errors(model, train_input, train_target)\n",
    "    nb_test_errors = compute_nb_errors(model, test_input, test_target)\n",
    "\n",
    "    train_accuracy = (100 * (\n",
    "        train_input.size(0)-nb_train_errors)) / train_input.size(0)\n",
    "\n",
    "    test_accuracy = (100 * (\n",
    "        test_input.size(0) - nb_test_errors)) / test_input.size(0)\n",
    "\n",
    "    train_error = 100 - train_accuracy\n",
    "    test_error = 100 - test_accuracy\n",
    "\n",
    "    model_result_attributes[id_num] = {\"epoch_loss\": model_loss,\n",
    "                                       \"train_accuracy\": train_accuracy,\n",
    "                                       \"train_error\": test_accuracy,\n",
    "                                       \"test_accuracy\": train_error,\n",
    "                                       \"test_error\": test_error}\n",
    "\n",
    "    print(\n",
    "        '\\nTrain Accuracy {:0.2f}% {:d}/{:d}'.format(\n",
    "            train_accuracy, (train_input.size(0) - nb_train_errors),\n",
    "            train_input.size(0)))\n",
    "\n",
    "    print(\n",
    "        'Test Accuracy {:0.2f}% {:d}/{:d}'.format(\n",
    "            test_accuracy, (test_input.size(0) - nb_test_errors),\n",
    "            test_input.size(0)))\n",
    "\n",
    "    print(\n",
    "        'Train Error {:0.2f}% {:d}/{:d}'.format(\n",
    "            train_error, nb_train_errors, train_input.size(0)))\n",
    "\n",
    "    print('Test Error {:0.2f}% {:d}/{:d}'.format(\n",
    "        test_error, nb_test_errors, test_input.size(0)))\n",
    "\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-elimination",
   "metadata": {},
   "source": [
    "# 4. Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "environmental-finding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x22d910d0e20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, nb_epochs+1)\n",
    "\n",
    "for mnum, mattr in model_result_attributes.items():\n",
    "    plt.plot(epochs, mattr[\"epoch_loss\"], label=\"Model\" + str(mnum))\n",
    "\n",
    "plt.title(\"Training Loss curve\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-costs",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch 0 Training loss 0.24819576740264893\n",
      "\tEpoch 100 Training loss 0.24715721607208252\n",
      "\tEpoch 200 Training loss 0.24572522938251495\n",
      "\tEpoch 300 Training loss 0.24388673901557922\n",
      "\tEpoch 400 Training loss 0.24142660200595856\n",
      "\tEpoch 500 Training loss 0.23788151144981384\n",
      "\tEpoch 600 Training loss 0.23215150833129883\n",
      "\tEpoch 700 Training loss 0.22063684463500977\n",
      "\tEpoch 800 Training loss 0.18469606339931488\n",
      "\tEpoch 900 Training loss 0.084800586104393\n",
      "\tEpoch 0 Training loss 0.25153446197509766\n",
      "\tEpoch 100 Training loss 0.24966003000736237\n",
      "\tEpoch 200 Training loss 0.248119056224823\n",
      "\tEpoch 300 Training loss 0.24636498093605042\n",
      "\tEpoch 400 Training loss 0.24404650926589966\n",
      "\tEpoch 500 Training loss 0.24065212905406952\n",
      "\tEpoch 600 Training loss 0.23477640748023987\n",
      "\tEpoch 700 Training loss 0.22147075831890106\n",
      "\tEpoch 800 Training loss 0.17438159883022308\n",
      "\tEpoch 900 Training loss 0.07708434760570526\n",
      "\tEpoch 0 Training loss 0.252061128616333\n",
      "\tEpoch 100 Training loss 0.24813802540302277\n",
      "\tEpoch 200 Training loss 0.2457977533340454\n",
      "\tEpoch 300 Training loss 0.24393533170223236\n",
      "\tEpoch 400 Training loss 0.241988405585289\n",
      "\tEpoch 500 Training loss 0.23930400609970093\n",
      "\tEpoch 600 Training loss 0.23464591801166534\n",
      "\tEpoch 700 Training loss 0.22441904246807098\n",
      "\tEpoch 800 Training loss 0.19145679473876953\n",
      "\tEpoch 900 Training loss 0.09810148924589157\n",
      "\tEpoch 0 Training loss 0.25099045038223267\n",
      "\tEpoch 100 Training loss 0.24878652393817902\n",
      "\tEpoch 200 Training loss 0.24703381955623627\n",
      "\tEpoch 300 Training loss 0.24529016017913818\n",
      "\tEpoch 400 Training loss 0.24326032400131226\n",
      "\tEpoch 500 Training loss 0.24048267304897308\n",
      "\tEpoch 600 Training loss 0.23593670129776\n",
      "\tEpoch 700 Training loss 0.2271706461906433\n",
      "\tEpoch 800 Training loss 0.20727773010730743\n",
      "\tEpoch 900 Training loss 0.16229228675365448\n",
      "\tEpoch 0 Training loss 0.2511199116706848\n",
      "\tEpoch 100 Training loss 0.24831335246562958\n",
      "\tEpoch 200 Training loss 0.24623320996761322\n",
      "\tEpoch 300 Training loss 0.24426697194576263\n",
      "\tEpoch 400 Training loss 0.24196340143680573\n",
      "\tEpoch 500 Training loss 0.238644540309906\n",
      "\tEpoch 600 Training loss 0.23304516077041626\n",
      "\tEpoch 700 Training loss 0.22334788739681244\n",
      "\tEpoch 800 Training loss 0.20796366035938263\n",
      "\tEpoch 900 Training loss 0.1822221577167511\n",
      "\tEpoch 0 Training loss 0.25088223814964294\n",
      "\tEpoch 100 Training loss 0.24844104051589966\n",
      "\tEpoch 200 Training loss 0.2467089593410492\n",
      "\tEpoch 300 Training loss 0.24501819908618927\n",
      "\tEpoch 400 Training loss 0.24306154251098633\n",
      "\tEpoch 500 Training loss 0.24041271209716797\n",
      "\tEpoch 600 Training loss 0.236252561211586\n",
      "\tEpoch 700 Training loss 0.22892612218856812\n",
      "\tEpoch 800 Training loss 0.21531826257705688\n",
      "\tEpoch 900 Training loss 0.18629904091358185\n",
      "\tEpoch 0 Training loss 0.2500353753566742\n",
      "\tEpoch 100 Training loss 0.24815452098846436\n",
      "\tEpoch 200 Training loss 0.24630124866962433\n",
      "\tEpoch 300 Training loss 0.2440468668937683\n",
      "\tEpoch 400 Training loss 0.24099378287792206\n",
      "\tEpoch 500 Training loss 0.23642665147781372\n",
      "\tEpoch 600 Training loss 0.22884005308151245\n",
      "\tEpoch 700 Training loss 0.21526746451854706\n",
      "\tEpoch 800 Training loss 0.18576698005199432\n",
      "\tEpoch 900 Training loss 0.0998302772641182\n",
      "\tEpoch 0 Training loss 0.24947784841060638\n",
      "\tEpoch 100 Training loss 0.24705247581005096\n",
      "\tEpoch 200 Training loss 0.24487142264842987\n",
      "\tEpoch 300 Training loss 0.2423732876777649\n",
      "\tEpoch 400 Training loss 0.23866060376167297\n",
      "\tEpoch 500 Training loss 0.2302895337343216\n",
      "\tEpoch 600 Training loss 0.2008379101753235\n",
      "\tEpoch 700 Training loss 0.13591177761554718\n",
      "\tEpoch 800 Training loss 0.05095287784934044\n",
      "\tEpoch 900 Training loss 0.03062312863767147\n",
      "\tEpoch 0 Training loss 0.2496521919965744\n",
      "\tEpoch 100 Training loss 0.24813418090343475\n",
      "\tEpoch 200 Training loss 0.24669188261032104\n",
      "\tEpoch 300 Training loss 0.2450941801071167\n",
      "\tEpoch 400 Training loss 0.24307386577129364\n",
      "\tEpoch 500 Training loss 0.24021406471729279\n",
      "\tEpoch 600 Training loss 0.23556232452392578\n",
      "\tEpoch 700 Training loss 0.22580160200595856\n",
      "\tEpoch 800 Training loss 0.19286216795444489\n",
      "\tEpoch 900 Training loss 0.0931062325835228\n",
      "\tEpoch 0 Training loss 0.25108203291893005\n",
      "\tEpoch 100 Training loss 0.24886034429073334\n",
      "\tEpoch 200 Training loss 0.24722065031528473\n",
      "\tEpoch 300 Training loss 0.24583113193511963\n",
      "\tEpoch 400 Training loss 0.24453845620155334\n",
      "\tEpoch 500 Training loss 0.24319292604923248\n",
      "\tEpoch 600 Training loss 0.24159620702266693\n",
      "\tEpoch 700 Training loss 0.2394341677427292\n",
      "\tEpoch 800 Training loss 0.2361023724079132\n",
      "\tEpoch 900 Training loss 0.22999195754528046\n",
      "\tEpoch 0 Training loss 0.2725713849067688\n",
      "\tEpoch 100 Training loss 0.23883643746376038\n",
      "\tEpoch 200 Training loss 0.18128429353237152\n",
      "\tEpoch 300 Training loss 0.07514958083629608\n",
      "\tEpoch 400 Training loss 0.04259265959262848\n",
      "\tEpoch 500 Training loss 0.0295039564371109\n",
      "\tEpoch 600 Training loss 0.022775214165449142\n",
      "\tEpoch 700 Training loss 0.019137226045131683\n",
      "\tEpoch 800 Training loss 0.016934996470808983\n",
      "\tEpoch 900 Training loss 0.015418651513755322\n",
      "\tEpoch 0 Training loss 0.25440138578414917\n",
      "\tEpoch 100 Training loss 0.1988838165998459\n",
      "\tEpoch 200 Training loss 0.09155965596437454\n",
      "\tEpoch 300 Training loss 0.041655778884887695\n",
      "\tEpoch 400 Training loss 0.027239542454481125\n",
      "\tEpoch 500 Training loss 0.02123096026480198\n",
      "\tEpoch 600 Training loss 0.017842667177319527\n",
      "\tEpoch 700 Training loss 0.01563873142004013\n",
      "\tEpoch 800 Training loss 0.014083663001656532\n",
      "\tEpoch 900 Training loss 0.012910675257444382\n",
      "\tEpoch 0 Training loss 0.25109824538230896\n",
      "\tEpoch 100 Training loss 0.2257528454065323\n",
      "\tEpoch 200 Training loss 0.14181669056415558\n",
      "\tEpoch 300 Training loss 0.06015228107571602\n",
      "\tEpoch 400 Training loss 0.03525535762310028\n",
      "\tEpoch 500 Training loss 0.02550315298140049\n",
      "\tEpoch 600 Training loss 0.020563418045639992\n",
      "\tEpoch 700 Training loss 0.017646048218011856\n",
      "\tEpoch 800 Training loss 0.015728123486042023\n",
      "\tEpoch 900 Training loss 0.014358550310134888\n",
      "\tEpoch 0 Training loss 0.2638012766838074\n",
      "\tEpoch 100 Training loss 0.22999124228954315\n",
      "\tEpoch 200 Training loss 0.1801992654800415\n",
      "\tEpoch 300 Training loss 0.09087249636650085\n",
      "\tEpoch 400 Training loss 0.051346130669116974\n",
      "\tEpoch 500 Training loss 0.03551511839032173\n",
      "\tEpoch 600 Training loss 0.02710019052028656\n",
      "\tEpoch 700 Training loss 0.022239934653043747\n",
      "\tEpoch 800 Training loss 0.019189374521374702\n",
      "\tEpoch 900 Training loss 0.01709570176899433\n",
      "\tEpoch 0 Training loss 0.2540031671524048\n",
      "\tEpoch 100 Training loss 0.22069111466407776\n",
      "\tEpoch 200 Training loss 0.12129395455121994\n",
      "\tEpoch 300 Training loss 0.048377737402915955\n",
      "\tEpoch 400 Training loss 0.029614659026265144\n",
      "\tEpoch 500 Training loss 0.021893834695219994\n",
      "\tEpoch 600 Training loss 0.017866941168904305\n",
      "\tEpoch 700 Training loss 0.015407362952828407\n",
      "\tEpoch 800 Training loss 0.013744061812758446\n",
      "\tEpoch 900 Training loss 0.012542142532765865\n",
      "\tEpoch 0 Training loss 0.2470664083957672\n",
      "\tEpoch 100 Training loss 0.2202625423669815\n",
      "\tEpoch 200 Training loss 0.16784748435020447\n",
      "\tEpoch 300 Training loss 0.06940937042236328\n",
      "\tEpoch 400 Training loss 0.03925160691142082\n",
      "\tEpoch 500 Training loss 0.027564670890569687\n",
      "\tEpoch 600 Training loss 0.021865399554371834\n",
      "\tEpoch 700 Training loss 0.018641948699951172\n",
      "\tEpoch 800 Training loss 0.01654396951198578\n",
      "\tEpoch 900 Training loss 0.015024988912045956\n",
      "\tEpoch 0 Training loss 0.2610425651073456\n",
      "\tEpoch 100 Training loss 0.20544841885566711\n",
      "\tEpoch 200 Training loss 0.08496055752038956\n",
      "\tEpoch 300 Training loss 0.04467063769698143\n",
      "\tEpoch 400 Training loss 0.029689131304621696\n",
      "\tEpoch 500 Training loss 0.02270587906241417\n",
      "\tEpoch 600 Training loss 0.019062593579292297\n",
      "\tEpoch 700 Training loss 0.016838574782013893\n",
      "\tEpoch 800 Training loss 0.015313382260501385\n",
      "\tEpoch 900 Training loss 0.01418244931846857\n",
      "\tEpoch 0 Training loss 0.25215089321136475\n",
      "\tEpoch 100 Training loss 0.23287104070186615\n",
      "\tEpoch 200 Training loss 0.20713351666927338\n",
      "\tEpoch 300 Training loss 0.17494899034500122\n",
      "\tEpoch 400 Training loss 0.12453191727399826\n",
      "\tEpoch 500 Training loss 0.07171885669231415\n",
      "\tEpoch 600 Training loss 0.046846408396959305\n",
      "\tEpoch 700 Training loss 0.034933023154735565\n",
      "\tEpoch 800 Training loss 0.027906564995646477\n",
      "\tEpoch 900 Training loss 0.02331559732556343\n",
      "\tEpoch 0 Training loss 0.245727077126503\n",
      "\tEpoch 100 Training loss 0.23469902575016022\n",
      "\tEpoch 200 Training loss 0.18623483180999756\n",
      "\tEpoch 300 Training loss 0.07886568456888199\n",
      "\tEpoch 400 Training loss 0.04355499893426895\n",
      "\tEpoch 500 Training loss 0.02995433658361435\n",
      "\tEpoch 600 Training loss 0.02316739782691002\n",
      "\tEpoch 700 Training loss 0.019400618970394135\n",
      "\tEpoch 800 Training loss 0.01706864684820175\n",
      "\tEpoch 900 Training loss 0.015453394502401352\n",
      "\tEpoch 0 Training loss 0.26209670305252075\n",
      "\tEpoch 100 Training loss 0.22495931386947632\n",
      "\tEpoch 200 Training loss 0.1363198608160019\n",
      "\tEpoch 300 Training loss 0.06017877534031868\n",
      "\tEpoch 400 Training loss 0.036674369126558304\n",
      "\tEpoch 500 Training loss 0.026235390454530716\n",
      "\tEpoch 600 Training loss 0.020952653139829636\n",
      "\tEpoch 700 Training loss 0.017873235046863556\n",
      "\tEpoch 800 Training loss 0.0158650204539299\n",
      "\tEpoch 900 Training loss 0.014438065700232983\n",
      "\tEpoch 0 Training loss 0.24845226109027863\n",
      "\tEpoch 100 Training loss 0.2442108392715454\n",
      "\tEpoch 200 Training loss 0.21965451538562775\n",
      "\tEpoch 300 Training loss 0.1420920342206955\n",
      "\tEpoch 400 Training loss 0.0723990797996521\n",
      "\tEpoch 500 Training loss 0.04330597445368767\n",
      "\tEpoch 600 Training loss 0.028834853321313858\n",
      "\tEpoch 700 Training loss 0.021518917754292488\n",
      "\tEpoch 800 Training loss 0.01726638153195381\n",
      "\tEpoch 900 Training loss 0.014518938027322292\n",
      "\tEpoch 0 Training loss 0.2485680729150772\n",
      "\tEpoch 100 Training loss 0.24357032775878906\n",
      "\tEpoch 200 Training loss 0.2086985558271408\n",
      "\tEpoch 300 Training loss 0.09666583687067032\n",
      "\tEpoch 400 Training loss 0.04058120399713516\n",
      "\tEpoch 500 Training loss 0.02415500394999981\n",
      "\tEpoch 600 Training loss 0.01792672835290432\n",
      "\tEpoch 700 Training loss 0.014647090807557106\n",
      "\tEpoch 800 Training loss 0.012719406746327877\n",
      "\tEpoch 900 Training loss 0.011463943868875504\n",
      "\tEpoch 0 Training loss 0.24925366044044495\n",
      "\tEpoch 100 Training loss 0.2448268085718155\n",
      "\tEpoch 200 Training loss 0.22045361995697021\n",
      "\tEpoch 300 Training loss 0.11687441170215607\n",
      "\tEpoch 400 Training loss 0.05128256976604462\n",
      "\tEpoch 500 Training loss 0.02969764545559883\n",
      "\tEpoch 600 Training loss 0.020792311057448387\n",
      "\tEpoch 700 Training loss 0.016483798623085022\n",
      "\tEpoch 800 Training loss 0.01408772449940443\n",
      "\tEpoch 900 Training loss 0.012451467104256153\n",
      "\tEpoch 0 Training loss 0.2501065731048584\n",
      "\tEpoch 100 Training loss 0.2474018931388855\n",
      "\tEpoch 200 Training loss 0.23629333078861237\n",
      "\tEpoch 300 Training loss 0.1553882658481598\n",
      "\tEpoch 400 Training loss 0.05285649746656418\n",
      "\tEpoch 500 Training loss 0.028474844992160797\n",
      "\tEpoch 600 Training loss 0.02004396729171276\n",
      "\tEpoch 700 Training loss 0.015947867184877396\n",
      "\tEpoch 800 Training loss 0.013815829530358315\n",
      "\tEpoch 900 Training loss 0.012844541110098362\n",
      "\tEpoch 0 Training loss 0.2507316470146179\n",
      "\tEpoch 100 Training loss 0.24859020113945007\n",
      "\tEpoch 200 Training loss 0.24296100437641144\n",
      "\tEpoch 300 Training loss 0.20196561515331268\n",
      "\tEpoch 400 Training loss 0.10198705643415451\n",
      "\tEpoch 500 Training loss 0.04436197876930237\n",
      "\tEpoch 600 Training loss 0.026526283472776413\n",
      "\tEpoch 700 Training loss 0.019044946879148483\n",
      "\tEpoch 800 Training loss 0.015237369574606419\n",
      "\tEpoch 900 Training loss 0.013017164543271065\n",
      "\tEpoch 0 Training loss 0.2508731186389923\n",
      "\tEpoch 100 Training loss 0.24691276252269745\n",
      "\tEpoch 200 Training loss 0.23163747787475586\n",
      "\tEpoch 300 Training loss 0.13610850274562836\n",
      "\tEpoch 400 Training loss 0.05050186067819595\n",
      "\tEpoch 500 Training loss 0.029462918639183044\n",
      "\tEpoch 600 Training loss 0.021195920184254646\n",
      "\tEpoch 700 Training loss 0.01639966294169426\n",
      "\tEpoch 800 Training loss 0.013528984040021896\n",
      "\tEpoch 900 Training loss 0.011652243323624134\n",
      "\tEpoch 0 Training loss 0.25004222989082336\n",
      "\tEpoch 100 Training loss 0.24458083510398865\n",
      "\tEpoch 200 Training loss 0.21445351839065552\n",
      "\tEpoch 300 Training loss 0.12389793992042542\n",
      "\tEpoch 400 Training loss 0.048640184104442596\n",
      "\tEpoch 500 Training loss 0.027445143088698387\n",
      "\tEpoch 600 Training loss 0.01966060698032379\n",
      "\tEpoch 700 Training loss 0.015825647860765457\n",
      "\tEpoch 800 Training loss 0.013665314763784409\n",
      "\tEpoch 900 Training loss 0.012205745093524456\n",
      "\tEpoch 0 Training loss 0.2505667209625244\n",
      "\tEpoch 100 Training loss 0.2457074224948883\n",
      "\tEpoch 200 Training loss 0.2260628193616867\n",
      "\tEpoch 300 Training loss 0.1448998600244522\n",
      "\tEpoch 400 Training loss 0.06625946611166\n",
      "\tEpoch 500 Training loss 0.03422485291957855\n",
      "\tEpoch 600 Training loss 0.021535739302635193\n",
      "\tEpoch 700 Training loss 0.01590682938694954\n",
      "\tEpoch 800 Training loss 0.013132890686392784\n",
      "\tEpoch 900 Training loss 0.01153822336345911\n",
      "\tEpoch 0 Training loss 0.2508912980556488\n",
      "\tEpoch 100 Training loss 0.24901896715164185\n",
      "\tEpoch 200 Training loss 0.24568401277065277\n",
      "\tEpoch 300 Training loss 0.23306715488433838\n",
      "\tEpoch 400 Training loss 0.15457071363925934\n",
      "\tEpoch 500 Training loss 0.05405046045780182\n",
      "\tEpoch 600 Training loss 0.02841482125222683\n",
      "\tEpoch 700 Training loss 0.01927950233221054\n",
      "\tEpoch 800 Training loss 0.01512613333761692\n",
      "\tEpoch 900 Training loss 0.012786789797246456\n",
      "\tEpoch 0 Training loss 0.24982978403568268\n",
      "\tEpoch 100 Training loss 0.24877145886421204\n",
      "\tEpoch 200 Training loss 0.2462061494588852\n",
      "\tEpoch 300 Training loss 0.2268294394016266\n",
      "\tEpoch 400 Training loss 0.13752847909927368\n",
      "\tEpoch 500 Training loss 0.06617528200149536\n",
      "\tEpoch 600 Training loss 0.040603555738925934\n",
      "\tEpoch 700 Training loss 0.028195971623063087\n",
      "\tEpoch 800 Training loss 0.020825767889618874\n",
      "\tEpoch 900 Training loss 0.016725799068808556\n",
      "\tEpoch 0 Training loss 0.3314511477947235\n",
      "\tEpoch 100 Training loss 0.1361226737499237\n",
      "\tEpoch 200 Training loss 0.05200311541557312\n",
      "\tEpoch 300 Training loss 0.02628498524427414\n",
      "\tEpoch 400 Training loss 0.017738662660121918\n",
      "\tEpoch 500 Training loss 0.013894336298108101\n",
      "\tEpoch 600 Training loss 0.011630629189312458\n",
      "\tEpoch 700 Training loss 0.009947631508111954\n",
      "\tEpoch 800 Training loss 0.008742527104914188\n",
      "\tEpoch 900 Training loss 0.007923903875052929\n",
      "\tEpoch 0 Training loss 0.3057630956172943\n",
      "\tEpoch 100 Training loss 0.14625969529151917\n",
      "\tEpoch 200 Training loss 0.06119588017463684\n",
      "\tEpoch 300 Training loss 0.028140347450971603\n",
      "\tEpoch 400 Training loss 0.019191663712263107\n",
      "\tEpoch 500 Training loss 0.014989439398050308\n",
      "\tEpoch 600 Training loss 0.012378556653857231\n",
      "\tEpoch 700 Training loss 0.011023230850696564\n",
      "\tEpoch 800 Training loss 0.009790189564228058\n",
      "\tEpoch 900 Training loss 0.008935646153986454\n",
      "\tEpoch 0 Training loss 0.3077005445957184\n",
      "\tEpoch 100 Training loss 0.1178770437836647\n",
      "\tEpoch 200 Training loss 0.035488370805978775\n",
      "\tEpoch 300 Training loss 0.021046675741672516\n",
      "\tEpoch 400 Training loss 0.016473805531859398\n",
      "\tEpoch 500 Training loss 0.013903448358178139\n",
      "\tEpoch 600 Training loss 0.011981946416199207\n",
      "\tEpoch 700 Training loss 0.010607154108583927\n",
      "\tEpoch 800 Training loss 0.009479749016463757\n",
      "\tEpoch 900 Training loss 0.008498540148139\n",
      "\tEpoch 0 Training loss 0.31277579069137573\n",
      "\tEpoch 100 Training loss 0.08164957910776138\n",
      "\tEpoch 200 Training loss 0.033489808440208435\n",
      "\tEpoch 300 Training loss 0.021624533459544182\n",
      "\tEpoch 400 Training loss 0.0168888159096241\n",
      "\tEpoch 500 Training loss 0.01427889708429575\n",
      "\tEpoch 600 Training loss 0.012498458847403526\n",
      "\tEpoch 700 Training loss 0.010741153731942177\n",
      "\tEpoch 800 Training loss 0.009548695757985115\n",
      "\tEpoch 900 Training loss 0.008325804024934769\n",
      "\tEpoch 0 Training loss 0.3208610415458679\n",
      "\tEpoch 100 Training loss 0.12192273139953613\n",
      "\tEpoch 200 Training loss 0.043923139572143555\n",
      "\tEpoch 300 Training loss 0.0245890524238348\n",
      "\tEpoch 400 Training loss 0.018015095964074135\n",
      "\tEpoch 500 Training loss 0.014923542737960815\n",
      "\tEpoch 600 Training loss 0.013206183910369873\n",
      "\tEpoch 700 Training loss 0.011600608937442303\n",
      "\tEpoch 800 Training loss 0.010347852483391762\n",
      "\tEpoch 900 Training loss 0.009235398843884468\n",
      "\tEpoch 0 Training loss 0.33345386385917664\n",
      "\tEpoch 100 Training loss 0.10309608280658722\n",
      "\tEpoch 200 Training loss 0.03888477385044098\n",
      "\tEpoch 300 Training loss 0.023513182997703552\n",
      "\tEpoch 400 Training loss 0.017677877098321915\n",
      "\tEpoch 500 Training loss 0.014348821714520454\n",
      "\tEpoch 600 Training loss 0.012224559672176838\n",
      "\tEpoch 700 Training loss 0.010738091543316841\n",
      "\tEpoch 800 Training loss 0.00965913012623787\n",
      "\tEpoch 900 Training loss 0.008847552351653576\n",
      "\tEpoch 0 Training loss 0.23073802888393402\n",
      "\tEpoch 100 Training loss 0.10076934844255447\n",
      "\tEpoch 200 Training loss 0.03820641338825226\n",
      "\tEpoch 300 Training loss 0.021700115874409676\n",
      "\tEpoch 400 Training loss 0.01577598787844181\n",
      "\tEpoch 500 Training loss 0.012255259789526463\n",
      "\tEpoch 600 Training loss 0.009981867857277393\n",
      "\tEpoch 700 Training loss 0.008554239757359028\n",
      "\tEpoch 800 Training loss 0.007565800100564957\n",
      "\tEpoch 900 Training loss 0.0067789894528687\n",
      "\tEpoch 0 Training loss 0.2577134370803833\n",
      "\tEpoch 100 Training loss 0.10218900442123413\n",
      "\tEpoch 200 Training loss 0.03897293657064438\n",
      "\tEpoch 300 Training loss 0.02438148483633995\n",
      "\tEpoch 400 Training loss 0.018245737999677658\n",
      "\tEpoch 500 Training loss 0.014318342320621014\n",
      "\tEpoch 600 Training loss 0.01184847205877304\n",
      "\tEpoch 700 Training loss 0.010390535928308964\n",
      "\tEpoch 800 Training loss 0.009273260831832886\n",
      "\tEpoch 900 Training loss 0.00839077215641737\n",
      "\tEpoch 0 Training loss 0.2653730511665344\n",
      "\tEpoch 100 Training loss 0.09944475442171097\n",
      "\tEpoch 200 Training loss 0.037840548902750015\n",
      "\tEpoch 300 Training loss 0.023586172610521317\n",
      "\tEpoch 400 Training loss 0.01790226623415947\n",
      "\tEpoch 500 Training loss 0.01463520247489214\n",
      "\tEpoch 600 Training loss 0.012202806770801544\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "mini_batch_size = 100\n",
    "\n",
    "model_loss_plots={1:[],2:[],3:[],4:[]}\n",
    "model_accuracies_train_plots={1:[],2:[],3:[],4:[]}\n",
    "model_accuracies_test_plots={1:[],2:[],3:[],4:[]}\n",
    "model_error_train_plots={1:[],2:[],3:[],4:[]}\n",
    "model_error_test_plots={1:[],2:[],3:[],4:[]}\n",
    "\n",
    "for iters in range(10):\n",
    "        losses=[]\n",
    "        accuracies=[]\n",
    "\n",
    "        model = Sequential(Linear(2, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 2), Sigmoid())\n",
    "\n",
    "\n",
    "        model_loss = train_model(model, train_input, train_target, mseloss,\n",
    "                                 learning_rate, mini_batch_size, nb_epochs)\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target)\n",
    "\n",
    "        train_accuracy = (100 * (\n",
    "            train_input.size(0)-nb_train_errors)) / train_input.size(0)\n",
    "\n",
    "        test_accuracy = (100 * (\n",
    "            test_input.size(0) - nb_test_errors)) / test_input.size(0)\n",
    "\n",
    "\n",
    "        model_loss_plots[1].append(model_loss)\n",
    "        model_accuracies_train_plots[1].append(train_accuracy)\n",
    "        model_accuracies_test_plots[1].append(test_accuracy)\n",
    "        model_error_train_plots[1].append(100-train_accuracy)\n",
    "        model_error_test_plots[1].append(100-test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "for iters in range(10):\n",
    "        losses=[]\n",
    "        accuracies=[]\n",
    "\n",
    "        model = Sequential(Linear(2, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 25, weightsinit=\"xavier\"), TanH(),\n",
    "                     Linear(25, 2, weightsinit=\"xavier\"), Sigmoid())\n",
    "\n",
    "\n",
    "        model_loss = train_model(model, train_input, train_target, mseloss,\n",
    "                                 learning_rate, mini_batch_size, nb_epochs)\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target)\n",
    "\n",
    "        train_accuracy = (100 * (\n",
    "            train_input.size(0)-nb_train_errors)) / train_input.size(0)\n",
    "\n",
    "        test_accuracy = (100 * (\n",
    "            test_input.size(0) - nb_test_errors)) / test_input.size(0)\n",
    "\n",
    "\n",
    "        model_loss_plots[2].append(model_loss)\n",
    "        model_accuracies_train_plots[2].append(train_accuracy)\n",
    "        model_accuracies_test_plots[2].append(test_accuracy)\n",
    "        model_error_train_plots[2].append(100-train_accuracy)\n",
    "        model_error_test_plots[2].append(100-test_accuracy)\n",
    "\n",
    "\n",
    "\n",
    "for iters in range(10):\n",
    "        losses=[]\n",
    "        accuracies=[]\n",
    "\n",
    "        model = Sequential(Linear(2, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 2), Sigmoid())\n",
    "\n",
    "\n",
    "        model_loss = train_model(model, train_input, train_target, mseloss,\n",
    "                                 learning_rate, mini_batch_size, nb_epochs)\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target)\n",
    "\n",
    "        train_accuracy = (100 * (\n",
    "            train_input.size(0)-nb_train_errors)) / train_input.size(0)\n",
    "\n",
    "        test_accuracy = (100 * (\n",
    "            test_input.size(0) - nb_test_errors)) / test_input.size(0)\n",
    "\n",
    "\n",
    "        model_loss_plots[3].append(model_loss)\n",
    "        model_accuracies_train_plots[3].append(train_accuracy)\n",
    "        model_accuracies_test_plots[3].append(test_accuracy)\n",
    "        model_error_train_plots[3].append(100-train_accuracy)\n",
    "        model_error_test_plots[3].append(100-test_accuracy)\n",
    "\n",
    "\n",
    "for iters in range(10):\n",
    "        losses=[]\n",
    "        accuracies=[]\n",
    "\n",
    "        model = Sequential(Linear(2, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                      Linear(25, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                      Linear(25, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                      Linear(25, 25, weightsinit=\"kaiming\"), ReLU(),\n",
    "                      Linear(25, 2, weightsinit=\"kaiming\"), Sigmoid())\n",
    "\n",
    "        model_loss = train_model(model, train_input, train_target, mseloss,\n",
    "                                 learning_rate, mini_batch_size, nb_epochs)\n",
    "\n",
    "        nb_train_errors = compute_nb_errors(model, train_input, train_target)\n",
    "        nb_test_errors = compute_nb_errors(model, test_input, test_target)\n",
    "\n",
    "        train_accuracy = (100 * (\n",
    "            train_input.size(0)-nb_train_errors)) / train_input.size(0)\n",
    "\n",
    "        test_accuracy = (100 * (\n",
    "            test_input.size(0) - nb_test_errors)) / test_input.size(0)\n",
    "\n",
    "\n",
    "        model_loss_plots[4].append(model_loss)\n",
    "        model_accuracies_train_plots[4].append(train_accuracy)\n",
    "        model_accuracies_test_plots[4].append(test_accuracy)\n",
    "        model_error_train_plots[4].append(100-train_accuracy)\n",
    "        model_error_test_plots[4].append(100-test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-clearing",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[1]\n",
    "c=1\n",
    "for loss in vals:\n",
    "    epochs=range(len(loss))\n",
    "    plt.plot(epochs, loss, 'lightblue',label=\"iteration\" + str(c))\n",
    "    c+=1\n",
    "\n",
    "\n",
    "a = np.array(vals)\n",
    "\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'blue',label=\"Average\")\n",
    "    \n",
    "plt.title(\"Training Loss curve for Model 1\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[2]\n",
    "c=1\n",
    "for loss in vals:\n",
    "    epochs=range(len(loss))\n",
    "    plt.plot(epochs, loss, 'pink',label=\"iteration\" + str(c))\n",
    "    c+=1\n",
    "\n",
    "\n",
    "a = np.array(vals)\n",
    "\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'red',label=\"Average\")\n",
    "    \n",
    "plt.title(\"Training Loss curve for Model 2\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[3]\n",
    "c=1\n",
    "for loss in vals:\n",
    "    epochs=range(len(loss))\n",
    "    plt.plot(epochs, loss, 'lightgreen',label=\"iteration\" + str(c))\n",
    "    c+=1\n",
    "\n",
    "\n",
    "a = np.array(vals)\n",
    "\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'green',label=\"Average\")\n",
    "    \n",
    "plt.title(\"Training Loss curve for Model 3\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-musician",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[3]\n",
    "c=1\n",
    "for loss in vals:\n",
    "    epochs=range(len(loss))\n",
    "    plt.plot(epochs, loss, 'violet',label=\"iteration\" + str(c))\n",
    "    c+=1\n",
    "\n",
    "\n",
    "a = np.array(vals)\n",
    "\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'purple',label=\"Average\")\n",
    "    \n",
    "plt.title(\"Training Loss curve for Model 4\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-latex",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[['lightblue','blue'],['pink','red'],['lightgreen','green'],['violet','purple']]\n",
    "for i in range(1,5):\n",
    "    vals=model_loss_plots[i]\n",
    "    c=1\n",
    "    for loss in vals:\n",
    "        epochs=range(len(loss))\n",
    "        plt.plot(epochs, loss, colors[i-1][0])\n",
    "        c+=1\n",
    "\n",
    "\n",
    "    a = np.array(vals)\n",
    "\n",
    "    res = np.average(a, axis=0)\n",
    "    plt.plot(epochs, res, colors[i-1][1],label=\"Model \"+str(i)+\" Average\")\n",
    "\n",
    "plt.title(\"Training Loss curves\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handy-overhead",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors=[['lightblue','blue'],['pink','red'],['lightgreen','green'],['violet','purple']]\n",
    "for i in range(1,5):\n",
    "    vals=model_loss_plots[i]\n",
    "    c=1\n",
    "    a = np.array(vals)\n",
    "\n",
    "    res = np.average(a, axis=0)\n",
    "    plt.plot(epochs, res, colors[i-1][1],label=\"Model \"+str(i)+\" Average\")\n",
    "\n",
    "plt.title(\"Training Loss curves\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radical-attitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[1]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'purple',label=\"Tanh\")\n",
    "\n",
    "vals=model_loss_plots[3]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'red',label=\"Relu\")\n",
    "\n",
    "    \n",
    "plt.title(\"Training Loss curve comparison Relu vs Tanh (uniform weight init)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-father",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[2]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'purple',label=\"Tanh\")\n",
    "\n",
    "vals=model_loss_plots[4]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'red',label=\"Relu\")\n",
    "\n",
    "    \n",
    "plt.title(\"Training Loss curve comparison Relu vs Tanh (kaiming and xavier weight init respectively)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[3]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'purple',label=\"uniform\")\n",
    "\n",
    "vals=model_loss_plots[4]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'red',label=\"kaiming\")\n",
    "\n",
    "    \n",
    "plt.title(\"Training Loss curve comparison for Relu (uniform vs kaiming weight init)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "vals=model_loss_plots[1]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'purple',label=\"uniform\")\n",
    "\n",
    "vals=model_loss_plots[2]\n",
    "a = np.array(vals)\n",
    "res = np.average(a, axis=0)\n",
    "plt.plot(epochs, res, 'red',label=\"xavier\")\n",
    "\n",
    "    \n",
    "plt.title(\"Training Loss curve comparison for Tanh (uniform vs xavier weight init)\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types_outputs=['Tanh with uniform weight initialization','Tanh with xavier weight initialization','Relu with uniform weight initialization','Relu with kaiming weight initialization']\n",
    "colors=[['lightblue','blue'],['pink','red'],['lightgreen','green'],['violet','purple']]\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10,8)   \n",
    "fig, ax = plt.subplots(nrows=2, ncols=2,constrained_layout=True)\n",
    "for train_out, subax, ind in zip(model_types_outputs, ax.flat,range(1,5)):\n",
    "    title_part1 = train_out\n",
    "    # plotting losses\n",
    "    subax.set_title(title_part1)\n",
    "    subax.set_xlabel(\"Epochs\")\n",
    "    subax.set_ylabel(\"Loss\")\n",
    "    plt.suptitle(\"Epoch v/s Loss of Models\")\n",
    "    epoch_loss_list = model_loss_plots[ind]\n",
    "    title_part1\n",
    "    for i in range(10):\n",
    "        subax.plot(epoch_loss_list[i], colors[ind-1][0],label=\"Iteration {}\".format(i+1))\n",
    "\n",
    "    average_loss = np.mean(model_loss_plots[ind], axis=0)\n",
    "    subax.plot(average_loss,colors[ind-1][1], label=\"Average Loss\")\n",
    "    subax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-enlargement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,8)\n",
    "# f, axes = plt.subplots(2, 2, sharey=True)\n",
    "\n",
    "df = pd.DataFrame(model_accuracies_train_plots)\n",
    "print(df)\n",
    "unstacked = df.unstack().to_frame()\n",
    "sns.boxplot(\n",
    "    x=unstacked.index.get_level_values(0),\n",
    "    y=unstacked[0]).set_title('Comparison of Model Train Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfied-restoration",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-builder",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monetary-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_accuracies_test_plots)\n",
    "print(df)\n",
    "unstacked = df.unstack().to_frame()\n",
    "sns.boxplot(\n",
    "    x=unstacked.index.get_level_values(0),\n",
    "    y=unstacked[0]).set_title('Comparison of Model Test Accuracies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-hampshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-cholesterol",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_error_train_plots)\n",
    "print(df)\n",
    "unstacked = df.unstack().to_frame()\n",
    "sns.boxplot(\n",
    "    x=unstacked.index.get_level_values(0),\n",
    "    y=unstacked[0]).set_title('Comparison of Model Train Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "previous-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(model_error_test_plots)\n",
    "print(df)\n",
    "unstacked = df.unstack().to_frame()\n",
    "sns.boxplot(\n",
    "    x=unstacked.index.get_level_values(0),\n",
    "    y=unstacked[0]).set_title('Comparison of Model Test Errors')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlikely-campaign",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passive-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-cambodia",
   "metadata": {},
   "source": [
    "# 5. Results\n",
    "The results for the models are as follows:\n",
    "\n",
    "| **Model Number** | **Activation** | **Loss**      | **Weight Initialisation** | **Training Accuracy** | **Testing Accuracy** | **Training Error** | **Testing Error** |\n",
    "|:----------------:|:--------------:|:-------------:|:-------------------------:|:---------------------:|:--------------------:|:------------------:|:-----------------:|\n",
    "| 1                | TanH           | MSE           | Uniform                   | 95.74%                | 93.98%               |2.07%               |2.52%              |\n",
    "| 2                | TanH           | MSE           | Xavier                    | 99.52%                | 99.54%               |0.35%               |0.66%              |\n",
    "| 3                | ReLU           | MSE           | Uniform                   | 99.72%                | 99.28%               |0.36%               |0.66%              |\n",
    "| 4                | ReLU           | MSE           | kaiming                   | 99.75%                | 98.06%               |0.18%               |0.92%              |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earned-occasions",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
