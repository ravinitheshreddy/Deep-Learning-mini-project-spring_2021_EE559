{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fewer-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Prologue\n",
    "import dlc_practical_prologue as prologue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "listed-census",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1f245974a30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#switching off autograd globally\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-asbestos",
   "metadata": {},
   "source": [
    "# 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-primary",
   "metadata": {},
   "source": [
    "## Baseclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "violent-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Base class for all modules.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Function to get the input, apply forward pass of module and\n",
    "        returns a tensor or a tuple of tensors..\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradswrtoutput):\n",
    "        \"\"\"\n",
    "        Function to get input the gradient of the loss with respect to the\n",
    "        module’s output, accumulate the gradient wrt the parameters, and\n",
    "        return a tensor or a tuple of tensors containing the gradient of\n",
    "        the loss wrt the module’s input.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Returns a list of pairs, each composed of a parameter tensor, and\n",
    "        a gradient tensor of same size.\n",
    "        \"\"\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-gravity",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-child",
   "metadata": {},
   "source": [
    "### TanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "higher-edition",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Module):\n",
    "    \"\"\"Module to apply the hyperbolic tangent function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying tanh to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the tanh should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the tanh on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.tanh()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying tanh\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * (1 - self.out_.pow(2))\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-sister",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "treated-barrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Module to apply the Rectified Linear function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying ReLU to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the ReLU should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the ReLU on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.clamp(min=0)\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying ReLU\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * self.out_(max=1)\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-protocol",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bearing-score",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did not require in the assignment\n",
    "\n",
    "class sigmoid(Module):\n",
    "    \"\"\"Module to apply the Sigmoid function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying sigmoid to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the sigmoid should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the sigmoid on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.sigmoid()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying sigmoid\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * (self.out_ - self.out_**2)\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-psychology",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "starting-summer",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "quantitative-dream",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    \"\"\"Module to calculate the Mean Squared Error.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the MSE Loss between input_ and target\n",
    "\n",
    "        Parameters:\n",
    "            input_ (Tensor): First tensor to calculate the MSE.\n",
    "            target (Tensor): Second tensor to calculate the MSE.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The Mean Squared Loss between input_ and target\n",
    "            \"\"\"\n",
    "        self.error = target - input_\n",
    "        self.out_ = self.error.pow(2).mean()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradswrtoutput=1):\n",
    "        \"\"\"\n",
    "        gradient of loss\n",
    "        \"\"\"\n",
    "        return (gradswrtoutput * 2 * self.error)/self.error.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exclusive-viking",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "addressed-alfred",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Base class for optimzers.\n",
    "    \"\"\"\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-alarm",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "inclusive-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Module to perform Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, params, lr=0.01):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Function to perform the single optimization step\n",
    "        \n",
    "        Parameters\n",
    "            params (list): List of the paramerters of the network\n",
    "            lr (float): The learning rate of the network\n",
    "        \"\"\"\n",
    "        \n",
    "        for weight, gradient in self.params:\n",
    "            if (weight is None) or (grad is None):\n",
    "                # incase of activation function modules, skip them\n",
    "                continue\n",
    "            else:\n",
    "                weight.add_(-self.lr*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-muslim",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-reunion",
   "metadata": {},
   "source": [
    "### Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adjacent-participation",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Module that implements as linear maxtrix operation layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, bias = True):\n",
    "        \"\"\"\n",
    "        Initialises the layer by creating empty weight and bias tensors\n",
    "        and Initialising them using Normal distribution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "        \n",
    "        self.w = torch.empty(in_features, out_features)\n",
    "        slef.gradw = torch.empty(in_features, out_features)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = torch.empty(out_features)\n",
    "            self.gradb = torch.empty(out_features)\n",
    "        else:\n",
    "            self.b = None\n",
    "            self.gradb = None\n",
    "            \n",
    "        self.initWeights()\n",
    "        \n",
    "    def initWeights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weight and bias parameters of the layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.w.normal_()\n",
    "        self.gradw.fill_(0)\n",
    "        \n",
    "        if slef.b is not None:\n",
    "            self.b.normal_()\n",
    "            self.gradb.fill_(0)\n",
    "            \n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the layer by multiplying the input with weights and adding the bias\n",
    "        \"\"\"\n",
    "        \n",
    "        self.inp = input_\n",
    "        \n",
    "        if self.b in None:\n",
    "            self.output = self.input.matmul(self.w)\n",
    "        else:\n",
    "            self.output = self.input.matmul(self.w).add(self.b)\n",
    "            \n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, gradwrtoutput):\n",
    "        \"\"\"\n",
    "        computes the gradient the weights and biases.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradw = self.input.t().matmul(gradwrtoutput)\n",
    "        self.gradw.add_(gradw)\n",
    "        \n",
    "        if self.b is not None:\n",
    "            gradb = gradwrtoutput.sum(0)\n",
    "            self.gradb.add_(gradb)\n",
    "        gradient = gradwrtoutput.matmul(self.w.t())\n",
    "        return gradient\n",
    "    \n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Return the paramerters of the layer\n",
    "        \"\"\"\n",
    "        \n",
    "        params = [(self.w, self.gradW)]\n",
    "        if self.b is not None:\n",
    "            params.append((self.b, self.gradb))\n",
    "        return params\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient to zero\n",
    "        \"\"\"\n",
    "        \n",
    "        self.gradw.zero_()\n",
    "        \n",
    "        if self.b is not None:\n",
    "            self.gradb.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-protest",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Module to hold the layers and build the Network\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A list to hold all layers of the network\n",
    "        self.modules = []\n",
    "        \n",
    "        for module in args:\n",
    "            self.modules.append(module)\n",
    "            \n",
    "        def forward(self, input_):\n",
    "            \"\"\"\n",
    "            DOCSTRING TBD\n",
    "            \"\"\"\n",
    "            self.inp = input_\n",
    "            # incase of no layers, the input itself is returned as output\n",
    "            output = input_\n",
    "            \n",
    "            for module in self.modules:\n",
    "                output = module.forward(output)\n",
    "            \n",
    "            self.output = output\n",
    "            \n",
    "            return self.output\n",
    "        \n",
    "        def backward(self, gradwrtoutput):\n",
    "            \"\"\"\n",
    "            DOCSTRING TBD\n",
    "            \"\"\"\n",
    "            \n",
    "            for module in reversed(self.modules):\n",
    "                gradwrtoutput = module.backward(gradwrtoutput)\n",
    "            \n",
    "            self.grad = gradwrtoutput\n",
    "            \n",
    "            return self.grad\n",
    "        \n",
    "        def param(self):\n",
    "            \"\"\"\n",
    "            List of parameters of all modules\n",
    "            \"\"\"\n",
    "            \n",
    "            params = []\n",
    "            for module in self.modules:\n",
    "                params.extend(module.param())\n",
    "            \n",
    "            return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "official-omega",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
