{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "presidential-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "measured-wireless",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x2b41c1ecdc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#switching off autograd globally\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "everyday-thickness",
   "metadata": {},
   "source": [
    "# 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-religion",
   "metadata": {},
   "source": [
    "## Baseclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "olive-attendance",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Base class for all modules.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Function to get the input, apply forward pass of module and\n",
    "        returns a tensor or a tuple of tensors..\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradswrtoutput):\n",
    "        \"\"\"\n",
    "        Function to get input the gradient of the loss with respect to the\n",
    "        module’s output, accumulate the gradient wrt the parameters, and\n",
    "        return a tensor or a tuple of tensors containing the gradient of\n",
    "        the loss wrt the module’s input.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Returns a list of pairs, each composed of a parameter tensor, and\n",
    "        a gradient tensor of same size.\n",
    "        \"\"\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-taylor",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-virus",
   "metadata": {},
   "source": [
    "### TanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "productive-world",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Module):\n",
    "    \"\"\"Module to apply the hyperbolic tangent function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying tanh to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the tanh should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the tanh on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.tanh()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying tanh\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * (1 - self.out_.pow(2))\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overall-scientist",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "modular-mitchell",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Module to apply the Rectified Linear function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying ReLU to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the ReLU should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the ReLU on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.clamp(min=0)\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying ReLU\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * self.out_(max=1)\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-denmark",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "modified-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did not require in the assignment\n",
    "\n",
    "class sigmoid(Module):\n",
    "    \"\"\"Module to apply the Sigmoid function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying sigmoid to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the sigmoid should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the sigmoid on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.sigmoid()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying sigmoid\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * (self.out_ - self.out_**2)\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appreciated-digit",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-roommate",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "innovative-talent",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    \"\"\"Module to calculate the Mean Squared Error.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the MSE Loss between input_ and target\n",
    "\n",
    "        Parameters:\n",
    "            input_ (Tensor): First tensor to calculate the MSE.\n",
    "            target (Tensor): Second tensor to calculate the MSE.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The Mean Squared Loss between input_ and target\n",
    "            \"\"\"\n",
    "        self.error = target - input_\n",
    "        self.out_ = self.error.pow(2).mean()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradswrtoutput=1):\n",
    "        \"\"\"\n",
    "        gradient of loss\n",
    "        \"\"\"\n",
    "        return (gradswrtoutput * 2 * self.error)/self.error.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-estonia",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "chinese-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Base class for optimzers.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-brunswick",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qualified-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Module to perform Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Function to perform the single optimization step\n",
    "\n",
    "        Parameters\n",
    "            params (list): List of the paramerters of the network\n",
    "            lr (float): The learning rate of the network\n",
    "        \"\"\"\n",
    "\n",
    "        for weight, gradient in self.params:\n",
    "            if (weight is None) or (gradient is None):\n",
    "                # incase of activation function modules, skip them\n",
    "                continue\n",
    "            else:\n",
    "                weight.add_(-self.lr*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-toronto",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "irish-munich",
   "metadata": {},
   "source": [
    "### Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "interpreted-beijing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Module that implements as linear maxtrix operation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        \"\"\"\n",
    "        Initialises the layer by creating empty weight and bias tensors\n",
    "        and Initialising them using Normal distribution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "\n",
    "        self.w = torch.empty(in_features, out_features)\n",
    "        self.gradw = torch.empty(in_features, out_features)\n",
    "\n",
    "        if bias:\n",
    "            self.b = torch.empty(out_features)\n",
    "            self.gradb = torch.empty(out_features)\n",
    "        else:\n",
    "            self.b = None\n",
    "            self.gradb = None\n",
    "\n",
    "        self.initWeights()\n",
    "\n",
    "    def initWeights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weight and bias parameters of the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.w.normal_()\n",
    "        self.gradw.fill_(0)\n",
    "\n",
    "        if self.b is not None:\n",
    "            self.b.normal_()\n",
    "            self.gradb.fill_(0)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the layer by multiplying the input with weights and adding the bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.inp = input_\n",
    "\n",
    "        if self.b in None:\n",
    "            self.output = self.input.matmul(self.w)\n",
    "        else:\n",
    "            self.output = self.input.matmul(self.w).add(self.b)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        \"\"\"\n",
    "        computes the gradient the weights and biases.\n",
    "        \"\"\"\n",
    "\n",
    "        gradw = self.input.t().matmul(gradwrtoutput)\n",
    "        self.gradw.add_(gradw)\n",
    "\n",
    "        if self.b is not None:\n",
    "            gradb = gradwrtoutput.sum(0)\n",
    "            self.gradb.add_(gradb)\n",
    "        gradient = gradwrtoutput.matmul(self.w.t())\n",
    "        return gradient\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Return the paramerters of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        params = [(self.w, self.gradW)]\n",
    "        if self.b is not None:\n",
    "            params.append((self.b, self.gradb))\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient to zero\n",
    "        \"\"\"\n",
    "\n",
    "        self.gradw.zero_()\n",
    "\n",
    "        if self.b is not None:\n",
    "            self.gradb.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-export",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "super-brighton",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Module to hold the layers and build the Network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "        # A list to hold all layers of the network\n",
    "        self.modules = []\n",
    "\n",
    "        for module in args:\n",
    "            self.modules.append(module)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        DOCSTRING TBD\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        # incase of no layers, the input itself is returned as output\n",
    "        output = input_\n",
    "\n",
    "        for module in self.modules:\n",
    "            output = module.forward(output)\n",
    "\n",
    "        self.output = output\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        \"\"\"\n",
    "        DOCSTRING TBD\n",
    "        \"\"\"\n",
    "\n",
    "        for module in reversed(self.modules):\n",
    "            gradwrtoutput = module.backward(gradwrtoutput)\n",
    "\n",
    "        self.grad = gradwrtoutput\n",
    "\n",
    "        return self.grad\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        List of parameters of all modules\n",
    "        \"\"\"\n",
    "\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.extend(module.param())\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-ethics",
   "metadata": {},
   "source": [
    "# 2. Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wireless-baptist",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_points):\n",
    "    \"\"\"\n",
    "    Common function to generate the dataset of 1,000 points sampled uniformly\n",
    "    in [0, 1]^2, each with a label 0 if outside the disk centered at (0.5; 0.5)\n",
    "    of radius 1/sqrt(2*pi), and 1 inside.\n",
    "\n",
    "    Parameters:\n",
    "        num_points (int): The number of points to be generated\n",
    "\n",
    "    Returns:\n",
    "        Tensor : A two dimensional input data with points sampled between [0,1]\n",
    "        Tensor : A one dimensional output data that contains labels\n",
    "        corresponding to the input data generated above\n",
    "    \"\"\"\n",
    "\n",
    "    input_ = torch.Tensor(num_points, 2).uniform_(0, 1)\n",
    "\n",
    "    labels = input_.sub(0.5).pow(2).sum(1).sub(1 / (2 * math.pi)).sign().add(1).div(2).long()\n",
    "\n",
    "    return input_, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-farming",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sophisticated-there",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, loss_criteria, learning_rate,\n",
    "                mini_batch_size, nb_epochs):\n",
    "\n",
    "    optimizer = SGD(model.param(), lr=learning_rate)\n",
    "\n",
    "    for epoch_number in range(nb_epochs):\n",
    "        acc_loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss_ = loss_criteria(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            acc_loss += loss_.loss.item()\n",
    "            model.zero_grad()\n",
    "            model.backward(loss_)\n",
    "            optimizer.step()\n",
    "        print(\"Epoch {} Training loss {} \". format(epoch_number, acc_loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
