{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "variable-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "provincial-active",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1d55c990130>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#switching off autograd globally\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "emotional-digest",
   "metadata": {},
   "source": [
    "# 1. Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-search",
   "metadata": {},
   "source": [
    "## Baseclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "particular-think",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \"\"\"\n",
    "    Base class for all modules.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Function to get the input, apply forward pass of module and\n",
    "        returns a tensor or a tuple of tensors..\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def backward(self, gradswrtoutput):\n",
    "        \"\"\"\n",
    "        Function to get input the gradient of the loss with respect to the\n",
    "        module’s output, accumulate the gradient wrt the parameters, and\n",
    "        return a tensor or a tuple of tensors containing the gradient of\n",
    "        the loss wrt the module’s input.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Returns a list of pairs, each composed of a parameter tensor, and\n",
    "        a gradient tensor of same size.\n",
    "        \"\"\"\n",
    "        return []\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradients of a module to 0\n",
    "        \"\"\"\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-actress",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-investigator",
   "metadata": {},
   "source": [
    "### TanH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mathematical-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Module):\n",
    "    \"\"\"Module to apply the hyperbolic tangent function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying tanh to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the tanh should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the tanh on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.tanh()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying tanh\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * (1 - self.out_.pow(2))\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-tribe",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "objective-prerequisite",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \"\"\"Module to apply the Rectified Linear function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying ReLU to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the ReLU should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the ReLU on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out = self.inp.clamp(min=0.0)\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying ReLU\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        self.out[self.out < 0 ]  = 0\n",
    "        self.out[self.out > 0 ]  = 1\n",
    "        return gradwrtoutput * self.out\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simple-madonna",
   "metadata": {},
   "source": [
    "### Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "central-folks",
   "metadata": {},
   "outputs": [],
   "source": [
    "# did not require in the assignment\n",
    "\n",
    "class Sigmoid(Module):\n",
    "    \"\"\"Module to apply the Sigmoid function\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the tensor after applying sigmoid to the input and saves the\n",
    "        input to help in backward pass computation.\n",
    "\n",
    "        Parameters:\n",
    "            input (Tensor): The tensor on which the sigmoid should be applied\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The tensor obtained after applying the sigmoid on the input\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        self.out_ = self.inp.sigmoid()\n",
    "        return self.out_\n",
    "\n",
    "    def backward(self, gradwrtoutput: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Returns the gradient of loss with respect to the input on applying sigmoid\n",
    "\n",
    "        Parameters:\n",
    "            gradientwrtoutput (Tensor): gradient with respect to the output\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The gradient of the loss with respect to the input\n",
    "        \"\"\"\n",
    "        return gradwrtoutput * (self.out_ - self.out_**2)\n",
    "\n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-strike",
   "metadata": {},
   "source": [
    "## Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-caribbean",
   "metadata": {},
   "source": [
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aggressive-petroleum",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Module):\n",
    "    \"\"\"Module to calculate the Mean Squared Error.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input_: torch.Tensor,\n",
    "                target: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Returns the MSE Loss between input_ and target\n",
    "\n",
    "        Parameters:\n",
    "            input_ (Tensor): First tensor to calculate the MSE.\n",
    "            target (Tensor): Second tensor to calculate the MSE.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The Mean Squared Loss between input_ and target\n",
    "            \"\"\"\n",
    "        self.error = input_ - target\n",
    "        self.loss = self.error.pow(2).mean()\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, gradswrtoutput=1):\n",
    "        \"\"\"\n",
    "        gradient of loss\n",
    "        \"\"\"\n",
    "        return (gradswrtoutput * 2 * self.error)/self.error.size()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-zealand",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "otherwise-antenna",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    \"\"\"\n",
    "    Base class for optimzers.\n",
    "    \"\"\"\n",
    "\n",
    "    def step(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-mailman",
   "metadata": {},
   "source": [
    "### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "addressed-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD(Optimizer):\n",
    "    \"\"\"\n",
    "    Module to perform Stochastic Gradient Descent\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        super().__init__()\n",
    "\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        Function to perform the single optimization step\n",
    "\n",
    "        Parameters\n",
    "            params (list): List of the paramerters of the network\n",
    "            lr (float): The learning rate of the network\n",
    "        \"\"\"\n",
    "\n",
    "        for weight, gradient in self.params:\n",
    "            if (weight is None) or (gradient is None):\n",
    "                # incase of activation function modules, skip them\n",
    "                continue\n",
    "            else:\n",
    "                weight.add_(-self.lr*gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-tamil",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaging-ownership",
   "metadata": {},
   "source": [
    "### Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chemical-factory",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"\n",
    "    Module that implements as linear maxtrix operation layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        \"\"\"\n",
    "        Initialises the layer by creating empty weight and bias tensors\n",
    "        and Initialising them using Normal distribution.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_features, self.out_features = in_features, out_features\n",
    "\n",
    "        self.w = torch.empty(self.in_features, self.out_features)\n",
    "        self.gradw = torch.empty(self.in_features, self.out_features)\n",
    "\n",
    "        if bias:\n",
    "            self.b = torch.empty(self.out_features)\n",
    "            self.gradb = torch.empty(self.out_features)\n",
    "        else:\n",
    "            self.b = None\n",
    "            self.gradb = None\n",
    "\n",
    "        self.initWeights()\n",
    "\n",
    "    def initWeights(self):\n",
    "        \"\"\"\n",
    "        Initialises the weight and bias parameters of the layer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.w.normal_()\n",
    "        self.gradw.fill_(0)\n",
    "\n",
    "        if self.b is not None:\n",
    "            self.b.normal_()\n",
    "            self.gradb.fill_(0)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the layer by multiplying the input with weights and adding the bias\n",
    "        \"\"\"\n",
    "\n",
    "        self.inp = input_\n",
    "\n",
    "        if self.b is None:\n",
    "            self.output = self.inp.matmul(self.w)\n",
    "        else:\n",
    "            self.output = self.inp.matmul(self.w).add(self.b)\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        \"\"\"\n",
    "        computes the gradient the weights and biases.\n",
    "        \"\"\"\n",
    "\n",
    "        gradw = self.inp.t().matmul(gradwrtoutput)\n",
    "        self.gradw.add_(gradw)\n",
    "\n",
    "        if self.b is not None:\n",
    "            gradb = gradwrtoutput.sum(0)\n",
    "            self.gradb.add_(gradb)\n",
    "        gradient = gradwrtoutput.matmul(self.w.t())\n",
    "        return gradient\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        Return the paramerters of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        params = [(self.w, self.gradw)]\n",
    "        if self.b is not None:\n",
    "            params.append((self.b, self.gradb))\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient to zero\n",
    "        \"\"\"\n",
    "\n",
    "        self.gradw.zero_()\n",
    "\n",
    "        if self.b is not None:\n",
    "            self.gradb.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-minute",
   "metadata": {},
   "source": [
    "### Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "finished-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):\n",
    "    \"\"\"\n",
    "    Module to hold the layers and build the Network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args):\n",
    "        super().__init__()\n",
    "\n",
    "        # A list to hold all layers of the network\n",
    "        self.modules = [module for module in args]\n",
    "\n",
    "    def forward(self, input_):\n",
    "        \"\"\"\n",
    "        DOCSTRING TBD\n",
    "        \"\"\"\n",
    "        self.inp = input_\n",
    "        # incase of no layers, the input itself is returned as output\n",
    "        output = input_\n",
    "\n",
    "        for module in self.modules:\n",
    "            output = module.forward(output)\n",
    "\n",
    "        self.output = output\n",
    "\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, gradwrtoutput):\n",
    "        \"\"\"\n",
    "        DOCSTRING TBD\n",
    "        \"\"\"\n",
    "        # The error is propagated in the reverse (backward) direction\n",
    "        for module in reversed(self.modules):\n",
    "            gradwrtoutput = module.backward(gradwrtoutput)\n",
    "\n",
    "        self.grad = gradwrtoutput\n",
    "\n",
    "        return self.grad\n",
    "\n",
    "    def param(self):\n",
    "        \"\"\"\n",
    "        List of parameters of all modules\n",
    "        \"\"\"\n",
    "\n",
    "        params = []\n",
    "        for module in self.modules:\n",
    "            params.extend(module.param())\n",
    "\n",
    "        return params\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        Sets the gradient to zero of all modules\n",
    "        \"\"\"\n",
    "\n",
    "        for weight, gradient in self.param():\n",
    "            if (weight is None) or (gradient is None):\n",
    "                # incase of activation function modules, skip them\n",
    "                continue\n",
    "            else:\n",
    "                gradient.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-education",
   "metadata": {},
   "source": [
    "# 2. Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "earned-nancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_points):\n",
    "    \"\"\"\n",
    "    Common function to generate the dataset of 1,000 points sampled uniformly\n",
    "    in [0, 1]^2, each with a label 0 if outside the disk centered at (0.5; 0.5)\n",
    "    of radius 1/sqrt(2*pi), and 1 inside.\n",
    "\n",
    "    Parameters:\n",
    "        num_points (int): The number of points to be generated\n",
    "\n",
    "    Returns:\n",
    "        Tensor : A two dimensional input data with points sampled between [0,1]\n",
    "        Tensor : A two dimensional output data that contains labels\n",
    "        corresponding to the input data generated above as one hot encoded variable\n",
    "    \"\"\"\n",
    "\n",
    "    input_ = torch.Tensor(num_points, 2).uniform_(0, 1)\n",
    "\n",
    "    labels = input_.sub(0.5).pow(2).sum(1).sub(1 / (2 * math.pi)).sign().add(1).div(2).long()\n",
    "\n",
    "    labels_onehot = torch.empty(num_points, 2).fill_(0)\n",
    "    labels_onehot[:, 0] = labels == 0\n",
    "    labels_onehot[:, 1] = labels == 1\n",
    "\n",
    "    return input_, labels_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rocky-mason",
   "metadata": {},
   "source": [
    "# 3. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fiscal-packing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_input, train_target, loss_criteria, learning_rate,\n",
    "                mini_batch_size, nb_epochs):\n",
    "\n",
    "    optimizer = SGD(model.param(), lr=learning_rate)\n",
    "    losses = []\n",
    "    for epoch_number in range(nb_epochs):\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            loss_ = loss_criteria.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            model.zero_grad()\n",
    "            model.backward(loss_criteria.backward())\n",
    "            optimizer.step()\n",
    "        if epoch_number % 100 == 0:\n",
    "            print(\"Epoch {} Training loss {} \". format(epoch_number, loss_.item()))\n",
    "        losses.append(loss_.item())\n",
    "    return losses\n",
    "\n",
    "\n",
    "def compute_nb_errors(model, input_, target):\n",
    "    '''\n",
    "    Computes and returns the number of prediction mistakes\n",
    "    '''\n",
    "    nb_data_errors = 0\n",
    "\n",
    "    output = model.forward(input_)\n",
    "\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    _, actual = torch.max(target.data, 1)\n",
    "\n",
    "    for k in range(input_.size()[0]):\n",
    "        if actual.data[k] != predicted[k]:\n",
    "            nb_data_errors = nb_data_errors + 1\n",
    "    return nb_data_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accompanied-vancouver",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0084, -0.1382],\n",
       "        [ 1.0279,  0.5660],\n",
       "        [-1.0160,  1.6597],\n",
       "        [-0.2534, -0.0853],\n",
       "        [-1.4146,  0.7649],\n",
       "        [-0.8255,  1.2179],\n",
       "        [-0.2828,  1.5426],\n",
       "        [ 1.2773,  1.1569],\n",
       "        [ 1.3914,  1.0999],\n",
       "        [-0.1954, -1.4017],\n",
       "        [ 1.4385, -0.3401],\n",
       "        [ 1.5906,  1.5709],\n",
       "        [-0.4735,  0.5988],\n",
       "        [ 0.6824,  0.8362],\n",
       "        [ 1.1471,  1.7393],\n",
       "        [-1.1745, -0.9665],\n",
       "        [-1.1113, -0.0538],\n",
       "        [-0.9959, -1.2350],\n",
       "        [ 0.3377,  1.1353],\n",
       "        [ 0.5902, -0.5585],\n",
       "        [ 0.0652, -0.6855],\n",
       "        [ 0.6994,  0.0552],\n",
       "        [ 0.6899,  0.8159],\n",
       "        [ 0.2197,  0.7576],\n",
       "        [-0.6487, -1.6435],\n",
       "        [ 0.3561, -0.1901],\n",
       "        [ 0.1724, -0.0430],\n",
       "        [-0.8153,  0.4746],\n",
       "        [-1.2103, -0.9335],\n",
       "        [ 0.5604,  1.6939],\n",
       "        [ 0.3806, -0.0227],\n",
       "        [-0.2334,  1.2296],\n",
       "        [ 0.6450, -1.1669],\n",
       "        [-1.2468,  0.7892],\n",
       "        [-0.5457,  0.1966],\n",
       "        [ 0.9234, -0.4493],\n",
       "        [ 1.6802, -0.3453],\n",
       "        [-1.2887, -1.2955],\n",
       "        [ 0.4325,  0.6354],\n",
       "        [ 1.0192, -0.3274],\n",
       "        [-1.2942,  0.6273],\n",
       "        [-1.3738, -0.0677],\n",
       "        [ 0.0973, -0.7608],\n",
       "        [-1.6653,  0.2629],\n",
       "        [-1.1376, -0.0478],\n",
       "        [-0.6498,  0.4507],\n",
       "        [-0.0165, -0.9017],\n",
       "        [-1.2023, -1.6715],\n",
       "        [-0.9924,  1.4821],\n",
       "        [-1.1295,  0.4960],\n",
       "        [-0.9089,  1.2787],\n",
       "        [-1.6258, -1.6049],\n",
       "        [ 1.7445, -0.5866],\n",
       "        [-1.2892, -0.1130],\n",
       "        [-0.3768, -1.2225],\n",
       "        [ 0.5461,  1.5459],\n",
       "        [ 1.5523,  1.4879],\n",
       "        [ 0.9296,  1.6475],\n",
       "        [ 0.5706, -0.1505],\n",
       "        [-1.3935,  1.7528],\n",
       "        [ 0.5155,  1.6460],\n",
       "        [ 1.1089, -0.5586],\n",
       "        [-0.9059, -1.5524],\n",
       "        [-1.6719,  1.1808],\n",
       "        [-0.3974,  1.2775],\n",
       "        [-0.7257,  0.0043],\n",
       "        [-0.9779,  1.6325],\n",
       "        [-0.9584, -1.2342],\n",
       "        [ 0.1265, -0.8530],\n",
       "        [ 0.5481,  0.9653],\n",
       "        [ 1.2065, -1.3707],\n",
       "        [-0.2118, -1.2966],\n",
       "        [ 0.4786, -0.4486],\n",
       "        [-0.1948, -0.6772],\n",
       "        [-1.0662, -0.3574],\n",
       "        [-1.6405,  1.0025],\n",
       "        [ 1.6840, -1.2536],\n",
       "        [-0.3427, -0.8706],\n",
       "        [ 0.4568,  0.3964],\n",
       "        [-0.6124, -1.6007],\n",
       "        [ 0.4446, -1.3831],\n",
       "        [ 0.8525,  1.5830],\n",
       "        [ 0.1377, -0.1071],\n",
       "        [ 0.8444,  1.5718],\n",
       "        [-0.6821, -0.1433],\n",
       "        [-1.4539, -0.4757],\n",
       "        [ 0.9391, -0.6511],\n",
       "        [ 1.5966, -0.7223],\n",
       "        [ 0.0353,  0.2276],\n",
       "        [ 0.1924, -1.5156],\n",
       "        [-1.4993, -0.7119],\n",
       "        [-0.7294, -0.4748],\n",
       "        [ 0.8630, -1.3606],\n",
       "        [ 0.6759,  0.6288],\n",
       "        [-1.3349, -0.7661],\n",
       "        [ 0.3954, -1.0419],\n",
       "        [-1.2175, -1.0171],\n",
       "        [-0.7841,  0.9953],\n",
       "        [-0.0370,  0.8345],\n",
       "        [-0.8423,  0.5545],\n",
       "        [ 1.0810, -0.2616],\n",
       "        [-1.1547, -0.2396],\n",
       "        [ 0.8918, -1.2277],\n",
       "        [ 1.3528,  1.4283],\n",
       "        [-1.2644,  0.4901],\n",
       "        [ 0.8975, -0.9059],\n",
       "        [ 0.3718,  1.4164],\n",
       "        [-0.3464,  1.4136],\n",
       "        [ 1.3403,  0.4304],\n",
       "        [ 1.7317, -0.9354],\n",
       "        [ 1.6718, -0.1580],\n",
       "        [-0.0207, -1.6242],\n",
       "        [ 0.5949, -1.7233],\n",
       "        [ 0.6509, -0.1103],\n",
       "        [-0.1553, -1.0392],\n",
       "        [ 0.8250,  0.8292],\n",
       "        [-0.3096, -0.8580],\n",
       "        [ 0.1489,  0.4080],\n",
       "        [ 0.6687,  1.6433],\n",
       "        [-0.3375, -0.5621],\n",
       "        [ 0.1493, -1.2703],\n",
       "        [-0.3188,  1.5363],\n",
       "        [ 1.6637, -0.2928],\n",
       "        [-0.3531,  1.6222],\n",
       "        [ 1.4878,  1.2322],\n",
       "        [-0.6070,  1.1760],\n",
       "        [ 1.2715,  0.9168],\n",
       "        [ 1.4340, -1.1694],\n",
       "        [ 1.0423, -1.2638],\n",
       "        [-0.3329,  1.3658],\n",
       "        [-0.9569,  0.9880],\n",
       "        [ 0.0346,  1.5956],\n",
       "        [-1.4623, -1.4859],\n",
       "        [-0.6633, -0.1141],\n",
       "        [-0.6983,  0.2510],\n",
       "        [ 0.8732,  0.2994],\n",
       "        [ 0.8814, -0.3496],\n",
       "        [ 0.2116,  0.2783],\n",
       "        [-0.1289, -0.5796],\n",
       "        [-0.3758, -0.8660],\n",
       "        [ 1.2056,  1.7018],\n",
       "        [ 0.1058, -0.0674],\n",
       "        [-0.6460,  0.6084],\n",
       "        [ 1.5074, -0.9625],\n",
       "        [ 0.9354, -0.2384],\n",
       "        [ 0.3485,  0.4637],\n",
       "        [ 0.8329,  1.6628],\n",
       "        [-0.0869,  0.4253],\n",
       "        [-0.9003, -0.3120],\n",
       "        [ 1.4220, -0.1094],\n",
       "        [-1.5666,  1.5616],\n",
       "        [-0.1112,  1.5330],\n",
       "        [ 1.5742, -1.3163],\n",
       "        [ 1.3857,  1.5029],\n",
       "        [ 1.0967, -1.3457],\n",
       "        [-1.4986,  1.5092],\n",
       "        [-0.0372,  0.9545],\n",
       "        [ 1.2625,  0.1497],\n",
       "        [ 0.6662, -1.5106],\n",
       "        [-1.2092,  0.9579],\n",
       "        [ 0.6534,  0.9648],\n",
       "        [ 1.4265,  0.4851],\n",
       "        [ 0.3946, -1.1032],\n",
       "        [-1.7179, -0.6405],\n",
       "        [ 0.3639,  0.3106],\n",
       "        [-0.6944, -0.6423],\n",
       "        [ 1.0114, -1.4236],\n",
       "        [-0.5338,  1.6595],\n",
       "        [ 1.0240,  1.2489],\n",
       "        [ 0.0790,  0.0340],\n",
       "        [-1.2597,  1.5955],\n",
       "        [-0.7158,  0.7054],\n",
       "        [-0.8697,  0.4903],\n",
       "        [-1.2850,  0.9907],\n",
       "        [ 1.6922,  0.2654],\n",
       "        [-0.4733,  0.7044],\n",
       "        [-0.1268, -1.3104],\n",
       "        [ 0.0311,  0.9183],\n",
       "        [-1.0304, -1.5955],\n",
       "        [ 0.2179,  0.6769],\n",
       "        [ 0.9759,  0.6195],\n",
       "        [ 1.5228,  1.0514],\n",
       "        [-0.7596, -1.6806],\n",
       "        [ 1.3620,  0.2374],\n",
       "        [ 0.4655, -0.3028],\n",
       "        [ 1.6368,  0.5181],\n",
       "        [-1.6972,  0.5919],\n",
       "        [-0.4495, -0.6500],\n",
       "        [ 1.0436,  1.6033],\n",
       "        [ 1.1810,  0.4102],\n",
       "        [-0.2953,  0.3849],\n",
       "        [-1.4803, -1.2937],\n",
       "        [ 0.1874, -1.5296],\n",
       "        [ 1.1047, -1.0601],\n",
       "        [ 0.5148,  1.2911],\n",
       "        [-1.3107,  1.3075],\n",
       "        [ 0.0164,  0.7502],\n",
       "        [-0.7303,  0.8559],\n",
       "        [ 0.5953,  0.6979],\n",
       "        [-0.7420, -1.1816]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input, train_target = generate_data(1000)\n",
    "test_input, test_target = generate_data(200)\n",
    "\n",
    "mu, std = train_input.mean(), train_input.std()\n",
    "train_input.sub_(mu).div_(std)\n",
    "test_input.sub_(mu).div_(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "virgin-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_1 = Sequential(Linear(2, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 25), TanH(),\n",
    "                     Linear(25, 2), Sigmoid())\n",
    "\n",
    "Model_2 = Sequential(Linear(2, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 25), ReLU(),\n",
    "                     Linear(25, 2), Sigmoid())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sudden-contact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Epoch 0 Training loss 0.3733569383621216 \n",
      "Epoch 100 Training loss 0.19858893752098083 \n",
      "Epoch 200 Training loss 0.1228412613272667 \n",
      "Epoch 300 Training loss 0.06333176791667938 \n",
      "Epoch 400 Training loss 0.039705824106931686 \n",
      "Epoch 500 Training loss 0.03125762566924095 \n",
      "Epoch 600 Training loss 0.02532937377691269 \n",
      "Epoch 700 Training loss 0.020313816145062447 \n",
      "Epoch 800 Training loss 0.01774614490568638 \n",
      "Epoch 900 Training loss 0.01597529835999012 \n",
      "Train Accuracy 99.00% 990/1000\n",
      "Test Accuracy 99.00% 198/200\n",
      "Train Error 1.00% 990/1000\n",
      "Test Error 1.00% 2/200\n",
      "\n",
      "\n",
      "Model 2\n",
      "Epoch 0 Training loss 0.5042600035667419 \n",
      "Epoch 100 Training loss 0.48176339268684387 \n",
      "Epoch 200 Training loss 0.48003074526786804 \n",
      "Epoch 300 Training loss 0.35738566517829895 \n",
      "Epoch 400 Training loss 0.3452419340610504 \n",
      "Epoch 500 Training loss 0.3294805884361267 \n",
      "Epoch 600 Training loss 0.31494423747062683 \n",
      "Epoch 700 Training loss 0.3131677210330963 \n",
      "Epoch 800 Training loss 0.2990536093711853 \n",
      "Epoch 900 Training loss 0.13339047133922577 \n",
      "Train Accuracy 95.80% 958/1000\n",
      "Test Accuracy 95.00% 190/200\n",
      "Train Error 4.20% 958/1000\n",
      "Test Error 5.00% 10/200\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "nb_epochs = 1000\n",
    "model_loss = []\n",
    "mini_batch_size = 200\n",
    "for i, M in enumerate([Model_1, Model_2]):\n",
    "    loss_fnc = MSELoss()\n",
    "    print('Model', i+1)\n",
    "\n",
    "    model_loss.append(train_model(M, train_input, train_target, loss_fnc, learning_rate, mini_batch_size, nb_epochs))\n",
    "\n",
    "    nb_train_errors = compute_nb_errors(M, train_input, train_target)\n",
    "    nb_test_errors = compute_nb_errors(M, test_input, test_target)\n",
    "\n",
    "    print('Train Accuracy {:0.2f}% {:d}/{:d}'.format((100 * (train_input.size(0)-nb_train_errors)) / train_input.size(0),\n",
    "                                                     (train_input.size(0) - nb_train_errors), train_input.size(0)))\n",
    "\n",
    "    print('Test Accuracy {:0.2f}% {:d}/{:d}'.format((100 * (test_input.size(0) - nb_test_errors)) / test_input.size(0),\n",
    "                                                    (test_input.size(0) - nb_test_errors), test_input.size(0)))\n",
    "    \n",
    "    print('Train Error {:0.2f}% {:d}/{:d}'.format((100 * (nb_train_errors)) / train_input.size(0),\n",
    "                                                     (train_input.size(0) - nb_train_errors), train_input.size(0)))\n",
    "\n",
    "    print('Test Error {:0.2f}% {:d}/{:d}'.format((100 * (nb_test_errors)) / test_input.size(0),\n",
    "                                                    (nb_test_errors), test_input.size(0)))\n",
    "    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-texas",
   "metadata": {},
   "source": [
    "# 4. Plotting Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "extensive-december",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1d5775e92e0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = torch.arange(1,nb_epochs+1)\n",
    "plt.plot(epochs, model_loss[0], label = 'Tanh')\n",
    "plt.plot(epochs, model_loss[1], label = 'ReLU')\n",
    "plt.title(\"Training Loss curve\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-benefit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlc",
   "language": "python",
   "name": "dlc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
